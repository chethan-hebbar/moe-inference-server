{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":151783,"status":"ok","timestamp":1759054866661,"user":{"displayName":"Chethan Hebbar","userId":"04834784337549419856"},"user_tz":-330},"id":"42CxtJyKgvU5","outputId":"f0966b05-8683-4e45-ec42-e92b2569a199"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://download.pytorch.org/whl/cu121\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n","Collecting torchtext\n","  Downloading https://download.pytorch.org/whl/torchtext-0.16.2%2Bcpu-cp312-cp312-linux_x86_64.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n","Collecting portalocker\n","  Downloading https://download.pytorch.org/whl/portalocker-2.10.1-py3-none-any.whl (18 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torchtext) (4.67.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchtext) (2.32.4)\n","Collecting torch\n","  Downloading https://download.pytorch.org/whl/cu121/torch-2.2.0%2Bcu121-cp312-cp312-linux_x86_64.whl (757.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.2/757.2 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchtext) (2.0.2)\n","Collecting torchdata==0.7.1 (from torchtext)\n","  Downloading https://download.pytorch.org/whl/torchdata-0.7.1-py3-none-any.whl (184 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.4/184.4 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m109.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m111.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu12 (from nvidia-cudnn-cu12==9.10.2.21->torch)\n","  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.7.1.2->torch)\n","  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.12/dist-packages (from torchdata==0.7.1->torchtext) (2.5.0)\n","INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n","Collecting torchvision\n","  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp312-cp312-linux_x86_64.whl (7.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m128.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.0%2Bcu121-cp312-cp312-linux_x86_64.whl (7.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m128.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.1%2Bcu121-cp312-cp312-linux_x86_64.whl (7.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m130.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.0%2Bcu121-cp312-cp312-linux_x86_64.whl (7.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.18.1%2Bcu121-cp312-cp312-linux_x86_64.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m128.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.18.0%2Bcu121-cp312-cp312-linux_x86_64.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m124.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.17.2%2Bcu121-cp312-cp312-linux_x86_64.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hINFO: pip is still looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n","  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.17.1%2Bcu121-cp312-cp312-linux_x86_64.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m132.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.17.0%2Bcu121-cp312-cp312-linux_x86_64.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m128.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n","INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n","Collecting torchaudio\n","  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.0%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m109.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.1%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m107.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.0%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.3.1%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.3.0%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m115.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.2.2%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m106.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hINFO: pip is still looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n","  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.2.1%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.2.0%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (3.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (2025.8.3)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Installing collected packages: portalocker, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchdata, torchaudio, torchtext\n","  Attempting uninstall: nvidia-nvtx-cu12\n","    Found existing installation: nvidia-nvtx-cu12 12.6.77\n","    Uninstalling nvidia-nvtx-cu12-12.6.77:\n","      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.27.3\n","    Uninstalling nvidia-nccl-cu12-2.27.3:\n","      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n","    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.7.77\n","    Uninstalling nvidia-curand-cu12-10.3.7.77:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n","    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n","      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n","    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n","    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n","    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n","      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n","    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n","    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.8.0+cu126\n","    Uninstalling torch-2.8.0+cu126:\n","      Successfully uninstalled torch-2.8.0+cu126\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.23.0+cu126\n","    Uninstalling torchvision-0.23.0+cu126:\n","      Successfully uninstalled torchvision-0.23.0+cu126\n","  Attempting uninstall: torchdata\n","    Found existing installation: torchdata 0.11.0\n","    Uninstalling torchdata-0.11.0:\n","      Successfully uninstalled torchdata-0.11.0\n","  Attempting uninstall: torchaudio\n","    Found existing installation: torchaudio 2.8.0+cu126\n","    Uninstalling torchaudio-2.8.0+cu126:\n","      Successfully uninstalled torchaudio-2.8.0+cu126\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtune 0.6.1 requires torchdata==0.11.0, but you have torchdata 0.7.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 portalocker-2.10.1 torch-2.2.0+cu121 torchaudio-2.2.0+cu121 torchdata-0.7.1 torchtext-0.16.2+cpu torchvision-0.17.0+cu121\n"]}],"source":["!pip3 install torch torchtext torchvision torchaudio portalocker --index-url https://download.pytorch.org/whl/cu121"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1759054866677,"user":{"displayName":"Chethan Hebbar","userId":"04834784337549419856"},"user_tz":-330},"id":"gIvnVQ-Ankln","outputId":"ef849bb8-0825-49a1-9127-c1e4c49370b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing dataloader.py\n"]}],"source":["%%writefile dataloader.py\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","from torchtext.datasets import AG_NEWS\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","import warnings\n","\n","# Suppress the specific UserWarning from torch.tensor on a tensor\n","# We are handling this correctly with torch.stack now, but this is good practice.\n","warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"To copy construct from a tensor.*\")\n","\n","class AGNewsDataset(Dataset):\n","    \"\"\"\n","    A PyTorch Dataset class to handle the AG News data.\n","    It processes the raw text iterator into a list of tensors in the constructor.\n","    \"\"\"\n","    def __init__(self, data_iterator, vocab, tokenizer):\n","        self.data = []\n","        self.vocab = vocab\n","        self.tokenizer = tokenizer\n","\n","        # This loop consumes the iterator and stores the processed data in self.data\n","        for label, text in data_iterator:\n","            tokens = self.tokenizer(text)\n","            indices = self.vocab(tokens)\n","            indices_tensor = torch.tensor(indices, dtype=torch.long)\n","            label_tensor = torch.tensor(label - 1, dtype=torch.long)\n","            self.data.append((label_tensor, indices_tensor))\n","\n","    def __len__(self):\n","        \"\"\"Returns the total number of samples.\"\"\"\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"Returns the processed sample at a given index.\"\"\"\n","        return self.data[idx]\n","\n","def get_dataloaders_and_vocab(batch_size=64):\n","    \"\"\"\n","    Orchestrates the entire data loading process.\n","    Handles iterator exhaustion correctly by creating fresh iterators for each step.\n","\n","    Args:\n","        batch_size (int): The batch size for the DataLoaders.\n","\n","    Returns:\n","        tuple: A tuple containing (train_dataloader, test_dataloader, vocab)\n","    \"\"\"\n","    print(\"--- Starting Data Loading Process ---\")\n","\n","    # --- Step A: Setup Tokenizer ---\n","    tokenizer = get_tokenizer('basic_english')\n","\n","    # --- Step B: Build Vocabulary ---\n","    # We create a fresh iterator here specifically for building the vocabulary.\n","    # This iterator will be exhausted after this step.\n","    print(\"Building vocabulary...\")\n","    train_iter_for_vocab, _ = AG_NEWS(split=('train', 'test'))\n","\n","    def yield_tokens(data_iter):\n","        for _, text in data_iter:\n","            yield tokenizer(text)\n","\n","    vocab = build_vocab_from_iterator(yield_tokens(train_iter_for_vocab), specials=[\"<unk>\", \"<pad>\"])\n","    vocab.set_default_index(vocab[\"<unk>\"])\n","    print(f\"Vocabulary built. Size: {len(vocab)}\")\n","\n","    # --- Step C: Instantiate Datasets ---\n","    # We create a SECOND set of fresh iterators to pass to our Dataset class.\n","    print(\"Processing data and creating Dataset objects...\")\n","    train_iter_for_dataset, test_iter_for_dataset = AG_NEWS(split=('train', 'test'))\n","\n","    train_dataset = AGNewsDataset(train_iter_for_dataset, vocab, tokenizer)\n","    test_dataset = AGNewsDataset(test_iter_for_dataset, vocab, tokenizer)\n","    print(\"Dataset objects created.\")\n","\n","    # --- Step D: Define the Collate Function ---\n","    # This function defines how to combine a list of samples into a single batch.\n","    def collate_batch(batch):\n","        label_list, text_list = [], []\n","        for (_label, _text) in batch:\n","            label_list.append(_label)\n","            text_list.append(_text)\n","\n","        # Use torch.stack for labels, as it's the correct way to combine a list of tensors.\n","        labels_tensor = torch.stack(label_list)\n","\n","        # Use pad_sequence for text to handle variable lengths.\n","        texts_tensor = pad_sequence(text_list, batch_first=True, padding_value=vocab['<pad>'])\n","\n","        return texts_tensor, labels_tensor # Note: Returning (text, label) is more conventional\n","\n","    # --- Step E: Create DataLoaders ---\n","    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n","    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n","    print(\"--- Data Loading Process Complete ---\")\n","\n","    return train_dataloader, test_dataloader, vocab\n","\n","\n","if __name__ == '__main__':\n","    \"\"\"\n","    This block runs only when the script is executed directly.\n","    It's a good way to test that the file is working correctly on its own.\n","    \"\"\"\n","    print(\"Testing dataloader.py directly...\")\n","\n","    train_dl, test_dl, vocab_obj = get_dataloaders_and_vocab(batch_size=8)\n","\n","    print(f\"\\nVocabulary size: {len(vocab_obj)}\")\n","\n","    text_batch, labels_batch = next(iter(train_dl))\n","\n","    print(\"\\n--- Testing a single batch ---\")\n","    print(f\"Text batch shape: {text_batch.shape}\")\n","    print(f\"Labels batch shape: {labels_batch.shape}\")\n","\n","    print(\"\\nFirst text tensor in batch:\\n\", text_batch[0])\n","    print(\"\\nFirst label in batch:\", labels_batch[0])\n","    print(\"\\ndataloader.py test successful!\")"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1759054866684,"user":{"displayName":"Chethan Hebbar","userId":"04834784337549419856"},"user_tz":-330},"id":"PgfqkWpbroJ2","outputId":"dba5ed56-b32f-44f2-eab3-bbe3837005b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing expert.py\n"]}],"source":["%%writefile expert.py\n","import torch\n","import torch.nn as nn\n","\n","class Expert(nn.Module):\n","    \"\"\"\n","    A simple feed-forward network, which will serve as an 'expert' in our MoE layer.\n","    \"\"\"\n","    def __init__(self, d_model, d_hidden):\n","        \"\"\"\n","        Args:\n","            d_model (int): The input and output dimension of the model.\n","            d_hidden (int): The dimension of the hidden layer.\n","        \"\"\"\n","        super(Expert, self).__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(d_model, d_hidden),\n","            nn.ReLU(),\n","            nn.Linear(d_hidden, d_model)\n","        )\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass for the expert.\n","\n","        Args:\n","            x (torch.Tensor): The input tensor. Shape: [..., d_model]\n","\n","        Returns:\n","            torch.Tensor: The output tensor. Shape: [..., d_model]\n","        \"\"\"\n","        return self.net(x)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1759054866703,"user":{"displayName":"Chethan Hebbar","userId":"04834784337549419856"},"user_tz":-330},"id":"KwOS9Xm7wUXA","outputId":"222f2940-7d0b-4880-8997-50b3667bbcf5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing gating.py\n"]}],"source":["%%writefile gating.py\n","import torch\n","import torch.nn as nn\n","\n","class Gating(nn.Module):\n","    \"\"\"\n","    A simple linear layer that acts as the gating mechanism in the MoE.\n","    It decides which experts to route the tokens to.\n","    \"\"\"\n","    def __init__(self, d_model, num_experts):\n","        \"\"\"\n","        Args:\n","            d_model (int): The dimension of the input tokens.\n","            num_experts (int): The total number of experts in the MoE layer.\n","        \"\"\"\n","        super(Gating, self).__init__()\n","        self.gate = nn.Linear(d_model, num_experts)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass for the gating network.\n","\n","        Args:\n","            x (torch.Tensor): The input tensor from the Transformer.\n","                              Shape: [batch_size, seq_len, d_model]\n","\n","        Returns:\n","            torch.Tensor: The logits for each expert.\n","                          Shape: [batch_size, seq_len, num_experts]\n","        \"\"\"\n","        # The output of this layer will be the raw scores (logits) for each expert\n","        return self.gate(x)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1759054866729,"user":{"displayName":"Chethan Hebbar","userId":"04834784337549419856"},"user_tz":-330},"id":"JvDVQV0axYzU","outputId":"e15a2131-9d2d-4c0c-e9b7-2adda9302d4a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing moe_layer.py\n"]}],"source":["%%writefile moe_layer.py\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from expert import Expert\n","from gating import Gating\n","\n","class MoELayer(nn.Module):\n","    \"\"\"\n","    A Mixture of Experts layer.\n","    \"\"\"\n","    def __init__(self, d_model, d_hidden, num_experts, top_k):\n","        \"\"\"\n","        Args:\n","            d_model (int): The dimension of the input and output.\n","            d_hidden (int): The hidden dimension of each expert FFN.\n","            num_experts (int): The total number of experts.\n","            top_k (int): The number of experts to route each token to.\n","        \"\"\"\n","        super(MoELayer, self).__init__()\n","\n","        # Basic validation\n","        if top_k > num_experts:\n","            raise ValueError(\"top_k must be less than or equal to num_experts\")\n","\n","        self.d_model = d_model\n","        self.num_experts = num_experts\n","        self.top_k = top_k\n","\n","        # Instantiate the experts and the gating network\n","        self.experts = nn.ModuleList([Expert(d_model, d_hidden) for _ in range(num_experts)])\n","        self.gating = Gating(d_model, num_experts)\n","\n","    def compute_load_balancing_loss(self, gating_logits):\n","        \"\"\"\n","        Computes the load balancing loss for the MoE layer.\n","        This loss encourages the gating network to distribute tokens evenly across experts.\n","\n","        Args:\n","            gating_logits (torch.Tensor): The raw logits from the gating network.\n","                                      Shape: [batch_size * seq_len, num_experts]\n","\n","        Returns:\n","            torch.Tensor: A single scalar value representing the load balancing loss.\n","        \"\"\"\n","        #\n","        # The formula is: alpha * sum(f_i * P_i) for i in experts\n","        # f_i = fraction of tokens sent to expert i\n","        # P_i = average probability (gating value) for expert i over tokens sent to it\n","        #\n","\n","        # Calculate P_i: softmax over all logits\n","        gating_probs = F.softmax(gating_logits, dim=-1)\n","\n","        # Calculate f_i: mean of the one-hot encoding of the chosen expert\n","        # For top-k > 1, this is more complex. A simplification is to look at the prob distribution.\n","        # We can calculate the fraction of the \"load\" each expert gets.\n","        f_i = gating_probs.mean(dim=0)\n","\n","        # Calculate P_i: The mean of the probabilities assigned to each expert across all tokens.\n","        P_i = gating_probs.mean(dim=0)\n","\n","\n","        # The loss is the dot product of these two vectors, scaled by the number of experts.\n","        # This encourages the product (and thus both f_i and P_i) to be uniform.\n","        loss = self.num_experts * torch.sum(f_i * P_i)\n","        return loss\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass for the MoE layer.\n","\n","        Args:\n","            x (torch.Tensor): Input tensor. Shape: [batch_size, seq_len, d_model]\n","\n","        Returns:\n","            (This will be implemented tomorrow)\n","        \"\"\"\n","        # Reshape input for gating: [batch_size * seq_len, d_model]\n","        # This treats each token independently.\n","        batch_size, seq_len, d_model = x.shape\n","        x_reshaped = x.view(-1, d_model)\n","\n","        # Get gating logits: [batch_size * seq_len, num_experts]\n","        gating_logits = self.gating(x_reshaped)\n","\n","        # Get the top-k experts and their scores (gating values)\n","        # The scores are softmax-normalized logits for the top-k experts.\n","        # top_k_gating_values shape: [batch_size * seq_len, top_k]\n","        # top_k_indices shape: [batch_size * seq_len, top_k]\n","        top_k_gating_values, top_k_indices = torch.topk(gating_logits, self.top_k, dim=-1)\n","\n","        # Apply softmax to the top-k logits to get weights\n","        top_k_gating_values = F.softmax(top_k_gating_values, dim=-1)\n","\n","        # Create a flat tensor of token indices\n","        # This will be [0, 0, 1, 1, 2, 2, ...] for top_k=2\n","        # It helps us track which output belongs to which original token.\n","        token_indices = torch.arange(x_reshaped.size(0)).repeat_interleave(self.top_k)\n","\n","        # Create a flat tensor of the chosen expert indices for all tokens\n","        flat_expert_indices = top_k_indices.flatten()\n","\n","        # Create our dispatch mask. It's a binary matrix of shape\n","        # [batch_size * seq_len, num_experts].\n","        # Entry (i, j) is 1 if token i is routed to expert j, and 0 otherwise.\n","        dispatch_mask = torch.zeros(x_reshaped.size(0), self.num_experts, device=x.device).bool()\n","        dispatch_mask.scatter_(1, top_k_indices, True)\n","\n","        # The final output tensor, initialized to zeros\n","        final_output = torch.zeros_like(x_reshaped)\n","\n","        # Now, iterate through each expert.\n","        for i in range(self.num_experts):\n","          # Find the tokens that are routed to this expert\n","          expert_mask = dispatch_mask[:, i]\n","\n","          # If no tokens are routed to this expert, skip it.\n","          if not expert_mask.any():\n","            continue\n","\n","          # Get the indices of the tokens for this expert\n","          token_ids_for_expert = expert_mask.nonzero(as_tuple=True)[0]\n","\n","          # Get the actual input tokens for this expert\n","          inputs_for_expert = x_reshaped[token_ids_for_expert]\n","\n","          # Pass the tokens through the expert\n","          expert_output = self.experts[i](inputs_for_expert)\n","\n","          # Find the gating values associated with these tokens for this expert\n","          gating_values_for_expert = top_k_gating_values[dispatch_mask[:, i]]\n","\n","          # The gating values tensor is currently [num_tokens_for_expert, top_k].\n","          # We need to find which of the top_k is our current expert 'i'.\n","          # We create a mask for this.\n","          k_mask = (top_k_indices[expert_mask] == i)\n","\n","          # Apply the mask to get the single correct gating value for each token.\n","          correct_gating_values = gating_values_for_expert[k_mask]\n","\n","          # Multiply the expert output by the gating values (element-wise)\n","          weighted_output = expert_output * correct_gating_values.unsqueeze(-1)\n","\n","          # Add the weighted output to the final output tensor at the correct positions.\n","          # This is the \"combine\" step. We use index_add_ for an efficient scatter-add.\n","          final_output.index_add_(0, token_ids_for_expert, weighted_output)\n","\n","        # Reshape the final output back to the original input shape\n","        return final_output.view(batch_size, seq_len, d_model), gating_logits"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1759054866733,"user":{"displayName":"Chethan Hebbar","userId":"04834784337549419856"},"user_tz":-330},"id":"hgGWfSx-Hb8w","outputId":"e01547a3-853c-46a0-c60e-d1c6fdc83bb2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing model.py\n"]}],"source":["%%writefile model.py\n","import torch\n","import torch.nn as nn\n","import math\n","\n","from moe_layer import MoELayer\n","\n","# --- Positional Encoding: A standard, non-learnable component ---\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0).transpose(0, 1)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:x.size(0), :]\n","        return self.dropout(x)\n","\n","class TransformerEncoderLayerWithMoE(nn.Module):\n","    def __init__(self, d_model, nhead, d_hidden, num_experts, top_k, dropout=0.1):\n","        super().__init__()\n","        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n","        self.moe_layer = MoELayer(d_model, d_hidden, num_experts, top_k)\n","\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, src):\n","        # Attention block\n","        attn_output, _ = self.self_attn(src, src, src)\n","        src = src + self.dropout(attn_output)\n","        src = self.norm1(src)\n","\n","        # MoE block\n","        moe_output, gating_logits = self.moe_layer(src)\n","        src = src + self.dropout(moe_output)\n","        src = self.norm2(src)\n","        return src, gating_logits\n","\n","# --- The Full Classification Model ---\n","class MoETransformerClassifier(nn.Module):\n","    def __init__(self, vocab_size, d_model, nhead, d_hidden, num_experts, top_k, num_classes, num_layers):\n","        super(MoETransformerClassifier, self).__init__()\n","        self.d_model = d_model\n","        self.embedding = nn.Embedding(vocab_size, d_model)\n","        self.pos_encoder = PositionalEncoding(d_model)\n","\n","        # Stack of our custom MoE-enabled encoder layers\n","        self.transformer_encoder = nn.ModuleList(\n","            [TransformerEncoderLayerWithMoE(d_model, nhead, d_hidden, num_experts, top_k) for _ in range(num_layers)]\n","        )\n","\n","        # The final classification head\n","        self.classifier = nn.Linear(d_model, num_classes)\n","\n","    def forward(self, src):\n","        # src shape: [batch_size, seq_len]\n","        src = self.embedding(src) * math.sqrt(self.d_model)\n","        src = self.pos_encoder(src)\n","\n","        all_gating_logits = []\n","\n","        # Pass through the stack of encoder layers\n","        for layer in self.transformer_encoder:\n","            src, gating_logits = layer(src)\n","            all_gating_logits.append(gating_logits)\n","\n","        # Pooling: Average the outputs of all tokens in the sequence\n","        pooled_output = src.mean(dim=1)\n","\n","        # Final classification\n","        output_logits = self.classifier(pooled_output)\n","        return output_logits, all_gating_logits"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1759054866737,"user":{"displayName":"Chethan Hebbar","userId":"04834784337549419856"},"user_tz":-330},"id":"HWSGNnfgiVNM","outputId":"c44b6467-95d4-489d-81c5-e2f93fbd1bf8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing train.py\n"]}],"source":["%%writefile train.py\n","import torch\n","import torch.nn as nn\n","from tqdm import tqdm\n","\n","from dataloader import get_dataloaders_and_vocab\n","from model import MoETransformerClassifier\n","import os\n","\n","# --- Configuration ---\n","BATCH_SIZE = 32\n","NUM_EPOCHS = 6\n","LEARNING_RATE = 1e-4\n","D_MODEL = 128\n","NHEAD = 4\n","D_HIDDEN = 512\n","NUM_EXPERTS = 8\n","TOP_K = 2\n","NUM_LAYERS = 2\n","NUM_CLASSES = 4\n","LOAD_BALANCING_ALPHA = 0.005\n","\n","def train_one_epoch(model, dataloader, optimizer, criterion, device, LOAD_BALANCING_ALPHA):\n","    model.train()\n","    total_loss = 0\n","\n","    progress_bar = tqdm(dataloader, desc=f\"Training Epoch\")\n","\n","    for text_batch, labels_batch in progress_bar:\n","        text_batch, labels_batch = text_batch.to(device), labels_batch.to(device)\n","\n","        # 1. Forward pass\n","        # This returns the final classification logits and a list of gating logits from each MoE layer\n","        output_logits, all_gating_logits = model(text_batch)\n","\n","        # 2. Calculate main classification loss (Cross-Entropy)\n","        main_loss = criterion(output_logits, labels_batch)\n","\n","        # 3. Calculate and sum the load balancing loss across ALL MoE layers\n","        load_balancing_loss = 0\n","\n","        # We iterate through the list of logits and the list of encoder layers together.\n","        # all_gating_logits[i] corresponds to the logits from model.transformer_encoder[i].\n","        for i, logits in enumerate(all_gating_logits):\n","            moe_layer = model.transformer_encoder[i].moe_layer\n","            load_balancing_loss += moe_layer.compute_load_balancing_loss(logits)\n","\n","        # 4. Combine the losses\n","        # The total loss is the main classification loss plus the scaled sum of all load balancing losses.\n","        combined_loss = main_loss + (LOAD_BALANCING_ALPHA * load_balancing_loss)\n","\n","        # 5. Backward pass and optimization\n","        optimizer.zero_grad()\n","        combined_loss.backward()\n","        optimizer.step()\n","\n","        total_loss += combined_loss.item()\n","\n","        progress_bar.set_postfix(loss=combined_loss.item(), main_loss=main_loss.item())\n","\n","    return total_loss / len(dataloader)\n","\n","\n","def validate_one_epoch(model, dataloader, criterion, device):\n","    model.eval()\n","    total_loss = 0\n","    correct_predictions = 0\n","    total_samples = 0\n","\n","    # We don't need to compute gradients during validation\n","    with torch.no_grad():\n","        progress_bar = tqdm(dataloader, desc=\"Validation Epoch\")\n","        for text_batch, labels_batch in progress_bar:\n","            text_batch, labels_batch = text_batch.to(device), labels_batch.to(device)\n","\n","            # 1. Forward pass (we don't need the gating logits for validation)\n","            output_logits, _ = model(text_batch)\n","\n","            # 2. Calculate loss\n","            loss = criterion(output_logits, labels_batch)\n","            total_loss += loss.item()\n","\n","            # 3. Calculate accuracy\n","            # Get the predicted class by finding the index of the max logit\n","            _, predicted_labels = torch.max(output_logits, 1)\n","\n","            # Compare predicted labels with the true labels\n","            correct_predictions += (predicted_labels == labels_batch).sum().item()\n","            total_samples += labels_batch.size(0)\n","\n","    avg_loss = total_loss / len(dataloader)\n","    accuracy = correct_predictions / total_samples\n","    return avg_loss, accuracy\n","\n","\n","def save_model_components(model, save_dir=\"model_weights\"):\n","    \"\"\"\n","    Saves the full model and its individual components (gating and experts).\n","    \"\"\"\n","    print(f\"\\n--- Saving model components to '{save_dir}' ---\")\n","\n","    # Create the directory if it doesn't exist\n","    os.makedirs(save_dir, exist_ok=True)\n","\n","    # 1. Save the full model's state_dict\n","    full_model_path = os.path.join(save_dir, \"full_model.pth\")\n","    torch.save(model.state_dict(), full_model_path)\n","    print(f\"Full model saved to {full_model_path}\")\n","\n","    # 2. Save the components of each TransformerEncoderLayerWithMoE\n","    # We assume for this project that all MoE layers are identical,\n","    # so we'll just save the components from the first one.\n","    # In a real-world scenario with different MoE layers, you'd save them all.\n","\n","    # Let's get the first MoE encoder layer\n","    first_moe_encoder_layer = model.transformer_encoder[0]\n","\n","    # Save the gating network\n","    gating_path = os.path.join(save_dir, \"gating_network.pth\")\n","    torch.save(first_moe_encoder_layer.moe_layer.gating.state_dict(), gating_path)\n","    print(f\"Gating network saved to {gating_path}\")\n","\n","    # Save each expert individually\n","    for i, expert in enumerate(first_moe_encoder_layer.moe_layer.experts):\n","        expert_path = os.path.join(save_dir, f\"expert_{i}.pth\")\n","        torch.save(expert.state_dict(), expert_path)\n","        print(f\"Expert {i} saved to {expert_path}\")\n","\n","    embedding_path = os.path.join(save_dir, \"embedding_layer.pth\")\n","    torch.save(model.embedding.state_dict(), embedding_path)\n","    print(f\"Embedding layer saved to {embedding_path}\")\n","\n","    classifier_path = os.path.join(save_dir, \"classifier_head.pth\")\n","    torch.save(model.classifier.state_dict(), classifier_path)\n","    print(f\"Classifier head saved to {classifier_path}\")\n","\n","    print(\"--- All components saved successfully ---\")\n","\n","\n","def main():\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(f\"Using device: {device}\")\n","\n","    train_dl, test_dl, vocab = get_dataloaders_and_vocab(batch_size=BATCH_SIZE)\n","    vocab_size = len(vocab)\n","\n","    model = MoETransformerClassifier(\n","        vocab_size, D_MODEL, NHEAD, D_HIDDEN, NUM_EXPERTS, TOP_K, NUM_CLASSES, NUM_LAYERS\n","    ).to(device)\n","\n","\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n","    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n","\n","    print(\"--- Starting Training & Validation ---\")\n","    for epoch in range(1, NUM_EPOCHS + 1):\n","        print(f\"\\n--- Epoch {epoch}/{NUM_EPOCHS} ---\")\n","\n","        # Train for one epoch\n","        avg_train_loss = train_one_epoch(model, train_dl, optimizer, criterion, device, LOAD_BALANCING_ALPHA)\n","\n","        # Validate for one epoch\n","        avg_val_loss, val_accuracy = validate_one_epoch(model, test_dl, criterion, device)\n","\n","        scheduler.step()\n","\n","        # Print the results for the epoch\n","        print(f\"End of Epoch {epoch}:\")\n","        print(f\"\\tAverage Training Loss: {avg_train_loss:.4f}\")\n","        print(f\"\\tAverage Validation Loss: {avg_val_loss:.4f}\")\n","        print(f\"\\tValidation Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")\n","\n","    save_model_components(model)\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rGgtHZbzVelX","outputId":"e13010ed-0028-439c-cca3-22505e76d4bd","executionInfo":{"status":"ok","timestamp":1759055839816,"user_tz":-330,"elapsed":973077,"user":{"displayName":"Chethan Hebbar","userId":"04834784337549419856"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","A module that was compiled using NumPy 1.x cannot be run in\n","NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n","versions of NumPy, modules must be compiled with NumPy 2.0.\n","Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n","\n","If you are a user of the module, the easiest solution will be to\n","downgrade to 'numpy<2' or try to upgrade the affected module.\n","We expect that some modules will need time to support NumPy 2.\n","\n","Traceback (most recent call last):  File \"/content/train.py\", line 1, in <module>\n","    import torch\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/__init__.py\", line 1471, in <module>\n","    from .functional import *  # noqa: F403\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/functional.py\", line 9, in <module>\n","    import torch.nn.functional as F\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/__init__.py\", line 1, in <module>\n","    from .modules import *  # noqa: F403\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n","    from .transformer import TransformerEncoder, TransformerDecoder, \\\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n","    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n","  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n","Using device: cuda\n","--- Starting Data Loading Process ---\n","Building vocabulary...\n","Vocabulary built. Size: 95812\n","Processing data and creating Dataset objects...\n","Dataset objects created.\n","--- Data Loading Process Complete ---\n","--- Starting Training & Validation ---\n","\n","--- Epoch 1/6 ---\n","Training Epoch: 100% 3750/3750 [02:23<00:00, 26.11it/s, loss=0.27, main_loss=0.259]\n","Validation Epoch: 100% 238/238 [00:02<00:00, 82.02it/s]\n","End of Epoch 1:\n","\tAverage Training Loss: 0.6585\n","\tAverage Validation Loss: 0.4236\n","\tValidation Accuracy: 0.8549 (85.49%)\n","\n","--- Epoch 2/6 ---\n","Training Epoch: 100% 3750/3750 [02:26<00:00, 25.64it/s, loss=0.539, main_loss=0.529]\n","Validation Epoch: 100% 238/238 [00:02<00:00, 82.79it/s]\n","End of Epoch 2:\n","\tAverage Training Loss: 0.4094\n","\tAverage Validation Loss: 0.3599\n","\tValidation Accuracy: 0.8758 (87.58%)\n","\n","--- Epoch 3/6 ---\n","Training Epoch: 100% 3750/3750 [02:24<00:00, 26.01it/s, loss=0.3, main_loss=0.29]\n","Validation Epoch: 100% 238/238 [00:02<00:00, 80.45it/s]\n","End of Epoch 3:\n","\tAverage Training Loss: 0.3462\n","\tAverage Validation Loss: 0.3283\n","\tValidation Accuracy: 0.8892 (88.92%)\n","\n","--- Epoch 4/6 ---\n","Training Epoch: 100% 3750/3750 [02:24<00:00, 25.94it/s, loss=0.221, main_loss=0.211]\n","Validation Epoch: 100% 238/238 [00:02<00:00, 82.25it/s]\n","End of Epoch 4:\n","\tAverage Training Loss: 0.2999\n","\tAverage Validation Loss: 0.3142\n","\tValidation Accuracy: 0.8921 (89.21%)\n","\n","--- Epoch 5/6 ---\n","Training Epoch: 100% 3750/3750 [02:24<00:00, 25.90it/s, loss=0.543, main_loss=0.532]\n","Validation Epoch: 100% 238/238 [00:02<00:00, 82.82it/s]\n","End of Epoch 5:\n","\tAverage Training Loss: 0.2920\n","\tAverage Validation Loss: 0.3095\n","\tValidation Accuracy: 0.8947 (89.47%)\n","\n","--- Epoch 6/6 ---\n","Training Epoch: 100% 3750/3750 [02:24<00:00, 26.00it/s, loss=0.228, main_loss=0.218]\n","Validation Epoch: 100% 238/238 [00:02<00:00, 83.07it/s]\n","End of Epoch 6:\n","\tAverage Training Loss: 0.2852\n","\tAverage Validation Loss: 0.3082\n","\tValidation Accuracy: 0.8959 (89.59%)\n","\n","--- Saving model components to 'model_weights' ---\n","Full model saved to model_weights/full_model.pth\n","Gating network saved to model_weights/gating_network.pth\n","Expert 0 saved to model_weights/expert_0.pth\n","Expert 1 saved to model_weights/expert_1.pth\n","Expert 2 saved to model_weights/expert_2.pth\n","Expert 3 saved to model_weights/expert_3.pth\n","Expert 4 saved to model_weights/expert_4.pth\n","Expert 5 saved to model_weights/expert_5.pth\n","Expert 6 saved to model_weights/expert_6.pth\n","Expert 7 saved to model_weights/expert_7.pth\n","Embedding layer saved to model_weights/embedding_layer.pth\n","Classifier head saved to model_weights/classifier_head.pth\n","--- All components saved successfully ---\n"]}],"source":["!python train.py"]},{"cell_type":"code","source":[],"metadata":{"id":"ZNN4WkY-mhHr"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"L4","authorship_tag":"ABX9TyOB831QHG/+yQoBPLW5OtVy"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}